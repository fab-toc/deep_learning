{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to overfitting\n",
    "\n",
    "In the previous lab we have designed an MLP network for image classification, and we have trained it in a rather straightforward fashion, which in practice is a bit risky. Indeed, our model was designed and trained for minimizing the loss on the training set for a given number of epochs. As such, we might end up with a model that is very good at predicting the data from this training set, but is unable to _generalize_ to unseen/new data (e.g., from the test set). This behavior is called **overfitting** and is a major problem in deep learning, wich we adress in this script.\n",
    "\n",
    "<center><a href=\"https://medium.com/geekculture/investigating-underfitting-and-overfitting-70382835e45c\">\n",
    "    <img src=\"https://miro.medium.com/max/1400/1*OeJVQ7sEvOJGxfGSCpbzyA.png\" width=\"600\"></a></center>\n",
    "    \n",
    "More specifically, we study two common sources of overfitting:\n",
    "\n",
    "- Training for too many epochs.\n",
    "- Using an over-parametrized (i.e., too large) model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and model\n",
    "\n",
    "As in the previous lab, we work with the MNIST dataset and we use a simple 3-layer MLP. We provide the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4aa905739f432db0e20bb12581afb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb31e8f24e144a8b6d0d54674d951a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1ab68cc53b4778a31f2145bed39333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dd85d680674ae790b01729c630665b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "Processing...\n",
      "\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Define the data repository\n",
    "data_dir = \"data/\"\n",
    "\n",
    "# Load the MNIST training dataset (in this script we don't need the test set), and take a small subset\n",
    "data_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    data_dir, train=True, download=True, transform=data_transforms\n",
    ")\n",
    "num_classes = len(train_data.classes)\n",
    "train_data = Subset(train_data, torch.arange(2000))\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP classifier\n",
    "class MLPClassif(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLPClassif, self).__init__()\n",
    "        self.input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU())\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), nn.ReLU()\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.input_layer(x)\n",
    "        y = self.hidden_layer(y)\n",
    "        out = self.output_layer(y)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Initialization function\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The validation set\n",
    "\n",
    "Until now, we have splitted our data into two sets: training and testing. A key ingredient in avoiding overfitting is the usage of an additional _validation_ (or _development_) set. The data is then split into:\n",
    "\n",
    "- the training set, which is used to train the model's parameters.\n",
    "- the validation set, which is used to evaluate the capacity of the model to generalize to unseen data.\n",
    "- the test set, which is used to compare different baselines / methods after training.\n",
    "\n",
    "<center><a href=\"https://www.v7labs.com/blog/train-validation-test-set\">\n",
    "    <img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/61568656a13218cdde7f6166_training-data-validation-test.png\" width=\"400\"></a></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the validation set by splitting the training data into 2 subsets (80% training and 20% validation)\n",
    "n_train_examples = int(len(train_data) * 0.8)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "train_data, valid_data = random_split(train_data, [n_train_examples, n_valid_examples])\n",
    "\n",
    "print(len(train_data), len(valid_data))\n",
    "\n",
    "# Define the corresponding dataloaders\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with validation: early stopping\n",
    "\n",
    "A first simple approach to avoid overfitting is to monitor the performance of the model on the validation set over epochs when training. At each epoch we compute the loss on the validation set, and check how it behaves: if the validation loss decreases, then we can continue training. Conversely, if the validation loss increases, it means the model has overfitted, so we need to stop training. In practice, we run training with a specified maximum number of epochs, and we save the model which yields the lowest validation loss. This strategy is called _early stopping_.\n",
    "\n",
    "<center><a href=\"https://theaisummer.com/regularization/\">\n",
    "    <img src=\"https://theaisummer.com/static/7a6353ed78b045f32e4ac39b0b4d66d2/a878e/early-stopping.png\" width=\"400\"></a></center>\n",
    "\n",
    "\n",
    "**Note**: In practice, instead of computing the validation _loss/error_ (which ideally should decrease), we can compute the validation _accuracy_ (which ideally should increase). The concept is the same, but it is usually better to use a validation metric that is as close as possible or equal to the test metric (here: accuracy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def eval_mlp_classifier(model, eval_dataloader):\n",
    "    # Set the model in 'evaluation' mode (this disables some layers (batch norm, dropout...) which are not needed when testing)\n",
    "    model.eval()\n",
    "\n",
    "    # In evaluation phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "\n",
    "        # Iterate over the dataset using the dataloader\n",
    "        for images, labels in eval_dataloader:\n",
    "            # Get the predicted labels\n",
    "            images = images.reshape(images.shape[0], -1)\n",
    "            y_predicted = model(images)\n",
    "\n",
    "            # To get the predicted labels, we need to get the max over all possible classes\n",
    "            _, label_predicted = torch.max(y_predicted.data, 1)\n",
    "\n",
    "            # Compute accuracy: count the total number of samples, and the correct labels (compare the true and predicted labels)\n",
    "            total_labels += labels.size(0)\n",
    "            correct_labels += (label_predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct_labels / total_labels\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function is very similar to what we did in the previous lab (so you can reuse it). However, note that here we use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer, which is a more efficient algorithm than SGD (the pytorch function is very similar though: you just need to provide the model's parameters and the learning rate).\n",
    "\n",
    "Then, at the end of each epoch, compute the accuracy on the validation set (use the provided eval function): if the accuracy is increasing, then store the current model as the optimal one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the training function 'train_val_mlp_classifier' with validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the network and initialize the parameters for reproducibility\n",
    "input_size = train_data[0][0][0].shape[0] * train_data[0][0][0].shape[1]\n",
    "hidden_size = 8\n",
    "output_size = num_classes\n",
    "model = MLPClassif(input_size, hidden_size, output_size)\n",
    "torch.manual_seed(0)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Training\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "model_opt, train_losses, val_accuracies = train_val_mlp_classifier(\n",
    "    model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: plot the training loss and validation accuracy (on two different subplots)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q1**</span> Put the plot above (training loss and validation accuracy) in your report. What do you remark? Is it beneficial to do early stopping?\n",
    "\n",
    "**Note**: Here, we actually conducted training for all epochs, and we simply recorded the best performing model. Proper early stopping would mean that we interrupt training when no improvement is observed after the best validation loss has been obtained (this number of epochs is called a _patience_ parameter, since it corresponds to the \"waiting time\" after the best validation score is obtained).\n",
    "This allows to reduce the computational time in addition to yielding the best model.\n",
    "\n",
    "## Adjusting the model's capacity\n",
    "\n",
    "The number of parameters in the network (i.e., the model's _capacity_) is expected to have a major impact on performance. Too few parameters might yield bad performance (underfitting), while too many might hamper the ability of the network to generalize (overfitting).\n",
    "\n",
    "<center><a href=\"https://classic.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html\">\n",
    "    <img src=\"https://classic.d2l.ai/_images/capacity-vs-error.svg\" width=\"400\"></a></center>\n",
    "\n",
    "\n",
    "Here, we study it by varying the number of hidden layers and checking how the validation accuracy behaves.\n",
    "\n",
    "Below we consider a general classifier module, where the number of hidden layers is passed as an input parameter. To create and stack together several layers, we can use the [ModuleList](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html) object, along with a simple loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On this example, we stack 5 {Linear + Relu} layers\n",
    "list_layer = nn.ModuleList(\n",
    "    [nn.Sequential(nn.Linear(10, 10), nn.ReLU()) for l in range(5)]\n",
    ")\n",
    "print(list_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the MLP classifier'MLPClassif_var' (with '__init__' and 'forward' methods) with a variable number of hidden layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - Define the general parameters of the MLP classifier model and training (same as before)\n",
    "# - For a number of hidden layers equal to 1, 3, and 5:\n",
    "#      - Train the model\n",
    "#      - Get the maximum validation accuracy\n",
    "# - Plot the max accuracy as a function of the number of hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q2**</span> Put the plot above (test accuracy as a function of number of hidden layers) in your report. What do you observe? What's the best model / number of hidden layers to use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
