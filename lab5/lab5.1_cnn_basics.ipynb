{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution basics\n",
    "\n",
    "In this script we see the basics of convolution functions, their parameters, and the main operations/layers used in convolutional neural networks.\n",
    "\n",
    "Check the documentation for more details: https://pytorch.org/docs/stable/nn.html#convolution-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "\n",
    "Convolution is a mathematical operation that corresponds to **filtering** the data.\n",
    "\n",
    "For instance, in 1D convolution, we apply a 1D filter (or \"kernel\") to a signal (e.g., audio waveform, EEG / MEG, etc.). The kernel slides over the whole signal to compute the convolution product.\n",
    "\n",
    "<center><a href=\"https://e2eml.school/convolution_one_d.html\">\n",
    "    <img src=\"https://e2eml.school/images/conv1d/aa_copy.gif\"></a></center>\n",
    "\n",
    "In this lab we will work with images, so we manipulate 2D convolution. In this case, the filter is a small matrix that slides over the input image and is multiplied by it to produce the output image. The convolution is said to be in \"2D\" because the kernel slides over 2 dimensions (height and width).\n",
    "\n",
    "<center><a href=\"https://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/\">\n",
    "    <img src=\"http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif\"></a></center>\n",
    "\n",
    "On the example above, the input image (in green) is of size $5\\times5$. We apply a kernel of size $3\\times3$ with the following values:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1\\\\\n",
    "0 & 1 & 0\\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The kernel slides over the image: this means that we take a sub-block of size $3 \\times 3$ (in yellow). Then, we multiply the value of the pixel with the corresponding value of the kernel; we do that for all pixels in the block and sum the results. This yields the convolution product, which is stored in the output image (on the right).\n",
    "\n",
    "It's important to understand that applying convolution **reduces the size** of the input image.\n",
    "\n",
    "Note that there is also 3D convolution, where the kernel slides over 3 dimensions. For instance, in video the kernel is a \"cube\" that slides over height, width, and time. It's theoretically possible to define convolution in any dimension, but it's not really used in practice (most applications use 1D, 2D or 3D convolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the provided example image as black and white, and normalize it so the values range in [0, 1]\n",
    "image_np = io.imread('tdp.jpeg', as_gray=True)\n",
    "print(image_np.shape)\n",
    "image_np = image_np / np.max(image_np)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image_np, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Convert the image into a pytorch float tensor\n",
    "image_t = torch.tensor(image_np).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image tensor above has size [height, width].\n",
    "# However, in pytorch we usually manipulate batches of images of shape (batch_size, num_channels, height, width).\n",
    "# So, we have to add extra dimensions (unsqueeze) corresponding to the batch_size and num_channels\n",
    "image_t = image_t.unsqueeze(0).unsqueeze(0)\n",
    "print(image_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the convolution function, we specify:\n",
    "# - the number of input channels (= the depth of the input image, usually 1 for black and white images, and 3 for RGB color images\n",
    "# - the number of output channels (which is equal to the number of kernels)\n",
    "# - the kernel size (the dimension of the filter)\n",
    "# - weather we use bias or not\n",
    "num_channels_in = 1\n",
    "num_channels_out = 1\n",
    "kernel_size = 3\n",
    "my_conv = nn.Conv2d(num_channels_in, num_channels_out, kernel_size=kernel_size, bias=False)\n",
    "\n",
    "# You can print the kernel weights: when using Conv2d, the weights are randomly initialized\n",
    "print(my_conv.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to set the value of the kernel weights.\n",
    "# For instance, let us set all of them to 1\n",
    "my_conv.weight = nn.Parameter(torch.ones_like(my_conv.weight))\n",
    "print(my_conv.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the image to the convolution\n",
    "output = my_conv(image_t)\n",
    "\n",
    "# Squeeze the output to remove the useless dimensions (batch_size and num_channels), and go back to numpy\n",
    "output_np = output.squeeze().detach().numpy()\n",
    "\n",
    "# Plot the output image\n",
    "print(output_np.shape)\n",
    "plt.imshow(output_np, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we have set all the kernel weights to 1. Therefore, the convolution simply computes the sum of adjacent pixels in a small neighborhood (here : 3x3 squares). As a result, the output image is the same as the input, but slightly blurred because of summing the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - define a function that outputs a convolution layer as above:\n",
    "#    - 1 input and 1 output channel, no bias\n",
    "#    - with kernel size specified by the input of the function\n",
    "#    - with kernel weights set to 1.\n",
    "# - apply convolutions of size 5, 10 and 20 to the input image\n",
    "# - plot the output images and print their shape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q1**</span> Put the images above in your report. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use rectangular (= non-square) kernels, with a different lengths in height and width\n",
    "my_conv = nn.Conv2d(num_channels_in, num_channels_out, kernel_size=(1, 20))\n",
    "my_conv.weight = nn.Parameter(torch.ones_like(my_conv.weight), requires_grad=False)\n",
    "output = my_conv(image_t)\n",
    "output_np = output.squeeze().detach().numpy()\n",
    "\n",
    "print('Shape :', output_np.shape)\n",
    "plt.imshow(output_np, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution can be used for many image processing applications.\n",
    "# For instance, we can define a filter to detect edges in an image\n",
    "edge_filter = torch.tensor([[[[-0.5, 0., 0.5], [-1., 0., 1.], [-0.5, 0., 0.5]]]])\n",
    "\n",
    "# Then, we define a convolution and set the weights (=the kernel parameters) to this filter\n",
    "my_conv = nn.Conv2d(1, 1, kernel_size=3, bias=False)\n",
    "my_conv.weight = nn.Parameter(edge_filter, requires_grad=False)\n",
    "\n",
    "# And now we apply convolution to the input image and get the edges\n",
    "print(image_t.shape)\n",
    "output = my_conv(image_t)\n",
    "output_np = output.squeeze().detach().numpy()\n",
    "plt.imshow(output_np, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "As remarked above and in the introduction, applying convolution reduces the dimension of the image as an edge effect. However, it's sometimes good to control the output dimension (e.g., to keep it the same as the input dimension). To do this, we can use **padding**, which consists in adding extra zeros on the four sides of the image: by artifically increasing the input image, we compensate for the dimension reduction in the output.\n",
    "\n",
    "<center><a href=\"https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\">\n",
    "    <img src=\"https://miro.medium.com/max/395/1*1okwhewf5KCtIPaFib4XaA.gif\"></a></center>\n",
    "\n",
    "In the example above, we use ``padding=1`` to add one row of pixels with the value 0 (white pixels within dashed lines) on the edges of the original image (in blue). Then, when we compute the convolution, this compensates for the size reduction due to a kernel size of 3, so the output image (in green) has the same shape as the original image (in blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and apply a convolution without or with padding\n",
    "my_conv = nn.Conv2d(num_channels_in, num_channels_out, kernel_size=3, bias=False)\n",
    "output = my_conv(image_t)\n",
    "my_conv_padd = nn.Conv2d(num_channels_in, num_channels_out, kernel_size=3, padding=1, bias=False)\n",
    "output_padd = my_conv_padd(image_t)\n",
    "\n",
    "# Check the size\n",
    "print('Input shape :', image_t.shape)\n",
    "print('Output shape, no padding: ', output.shape)\n",
    "print('Output shape, with padding: ', output_padd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write a convolution with a kernel_size=5 (1 output channel, no bias)\n",
    "# How much padding do you need to keep the image size the same?\n",
    "# Same question with a non-square kernel of size=(9, 13) (padding can also be non-square)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride\n",
    "\n",
    "Sometimes, we don't want to process all the adjacent pixels in an image, but instead to \"jump\" from blocks of pixels to others. To do that, we can adjust the `stride` parameter of the convolution.\n",
    "\n",
    "<center><a href=\"https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\">\n",
    "    <img src=\"https://miro.medium.com/max/294/1*BMngs93_rm2_BpJFH2mS0Q.gif\"></a></center>\n",
    "\n",
    "On the illustration above, the stride is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a convolution with a kernel_size=1, weights equal to 1, but with a stride of 2:\n",
    "# it's equivalent to removing every other pixel (or down-sampling the image)\n",
    "my_conv = nn.Conv2d(num_channels_in, num_channels_out, kernel_size=1, stride=2, bias=False)\n",
    "my_conv.weight = nn.Parameter(torch.ones_like(my_conv.weight), requires_grad=False)\n",
    "output = my_conv(image_t)\n",
    "output_np = output.squeeze().detach().numpy()\n",
    "\n",
    "print('Shape : ', output_np.shape)\n",
    "plt.imshow(output_np, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: create and apply the same convolution as above, but with a stride of 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: same, but with non-square stride = (1, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "For image classification / object detection, it is common in CNNs to apply a pooling function after convolution. Indeed, convolution produces output images where the important features (e.g., the detected object) might be spread over the image. The goal of pooling is to obtain a more compact representation of the image to avoid this spread (a property known as \"local translation invariance\"). As a result, a classification network is less sensitive to shift/translation in the image. Since it reduces the size of the data by discarding some pixels, pooling is also useful to reduce the computational load and to reduce the risk of overfitting.\n",
    "\n",
    "<center><a href=\"https://medium.com/nerd-for-tech/convolutional-networks-b54335f4e21f\">\n",
    "    <img src=\"https://miro.medium.com/max/700/1*gaD6SJ6kQNVOclE_WkwLNQ.png\"></a></center>\n",
    "\n",
    "The example above uses a **maximum** pooling function (it keeps the maximum pixel) with a size of 2 and a stride of 2 (by default in pytorch, the stride is equal to the size of the pooling). There are other types of pooling, such as the average pooling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply a max pooling on the original image\n",
    "my_pool = nn.MaxPool2d(kernel_size = 10)\n",
    "output = my_pool(image_t)\n",
    "print(output.shape)\n",
    "\n",
    "# Plot the result: it should be a downsampled image\n",
    "output_np = output.squeeze().detach().numpy()\n",
    "plt.imshow(output_np, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q2**</span> In your report, put the images obtained with a stride of 10 and with a max pooling of size 10. Explain the difference between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
