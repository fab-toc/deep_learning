{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional autoencoders\n",
    "\n",
    "In this script we write a CNN autoencoder and apply it to the task of image desnoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import copy\n",
    "\n",
    "# Define the data repository\n",
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization function\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposed convolution\n",
    "\n",
    "As seen in the previous lab, convolution usually reduces the size of the image. However, in some applications (e.g., image synthesis from low-dimension features) it is usefull to increase it. That's notably needed for autoencoders, since after projecting the data into a compact latent representation, we need to expand this representation back into the image space.\n",
    "\n",
    "This is exactly what [transposed convolution](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d) does. Simply put, a transposed convolution adds some zeros inside the input image (and on the edge) to artificially increase the size.\n",
    "\n",
    "Convolution             |  Transposed convolution\n",
    ":-------------------------:|:-------------------------:\n",
    "![](https://miro.medium.com/max/294/1*BMngs93_rm2_BpJFH2mS0Q.gif)  |  ![](https://miro.medium.com/max/395/1*Lpn4nag_KRMfGkx1k6bV-g.gif)\n",
    "\n",
    "<center><a href=\"https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\">Source</a></center>\n",
    "\n",
    "On the left, we use convolution with a kernel size of 3 and stride of 2. On the right, we then use a transpoed convolution with the same parameters, and its effect is to procude an image with the same size as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'io' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-00642b56d403>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the provided example image as black and white, and normalize it so the values ranges in [0, 1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tdp.jpeg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_gray\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimage_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_np\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'io' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the provided example image as black and white, and normalize it so the values ranges in [0, 1]\n",
    "image_np = io.imread('tdp.jpeg', as_gray=True)\n",
    "print(image_np.shape)\n",
    "image_np = image_np / np.max(image_np)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image_np, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Convert the image into a pytorch float tensor (and unsqueeze it to the proper size)\n",
    "image_t = torch.tensor(image_np).float().unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels_in = 1\n",
    "num_channels_out = 1\n",
    "\n",
    "# First, let us apply a convolution with a kernel size of (3, 6) and a stride of 4.\n",
    "my_conv = nn.Conv2d(num_channels_in, num_channels_out, kernel_size=(3, 6), stride=4)\n",
    "output = my_conv(image_t)\n",
    "print('Original image: ', image_t.shape)\n",
    "print('Output of the convolution : ', output.shape)\n",
    "\n",
    "# Display the output after convolution\n",
    "plt.imshow(output.detach().squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we reproduce an image with the same size as the original input\n",
    "my_convt = nn.ConvTranspose2d(num_channels_in, num_channels_out, kernel_size=(3, 6), stride=4)\n",
    "image_convt = my_convt(output)\n",
    "print('After applying transposed convolution : ', image_convt.shape)\n",
    "\n",
    "# Display the output after transposed convolution\n",
    "plt.imshow(image_convt.detach().squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, transposed convolution does not invert convolution (it's not \"deconvolution\"): it only guarantees that the size will be equal to that of the image before convolution (but not its pixel values).\n",
    "\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "data_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_data = datasets.MNIST(data_dir, train=True, download=True, transform=data_transforms)\n",
    "test_data = datasets.MNIST(data_dir, train=False, download=True, transform=data_transforms)\n",
    "num_classes = len(train_data.classes)\n",
    "\n",
    "# Take a subset of the train/test data\n",
    "train_data = Subset(train_data, torch.arange(400))\n",
    "test_data = Subset(test_data, torch.arange(50))\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN autoencoder\n",
    "\n",
    "As we saw it in lab 4, autoencoders are networks that project the input data into a low-dimension space, and then expand this compact representation back into the input space. In CNN autoencoders, the encoder uses convolutions to reduce the image size, while the decoder uses transposed convolutions to expand it. We propose the following architecture:\n",
    "\n",
    "- The encoder consists of two layers, with a convolution function, a RELU and a max pooling. The convolution functions have a kernel size of 3 and a padding of 1. They use 16 and 4 output channels, respectively. The max pooling functions use a kernel size of 2.\n",
    "\n",
    "- The decoder consists of two layers, with a transposed convolution and an activation function (RELU for the first layer, Sigmoid for the second layer). The transposed convolution functions have a kernel size of 2 and a stride of 2. They use 16 and 1 output channels, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f4579d48e863>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TO DO: write the CNN autoencoder module ('__init__' and 'forward' methods)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mCNNAutoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCNNAutoencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# TO DO: write the CNN autoencoder module ('__init__' and 'forward' methods)\n",
    "\n",
    "class CNNAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNAutoencoder, self).__init__()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CNN autoencoder, initialize it and print the number of parameters\n",
    "model_cnn_ae = CNNAutoencoder()\n",
    "torch.manual_seed(0)\n",
    "model_cnn_ae.apply(init_weights)\n",
    "print('Total number of parameters:', sum(p.numel() for p in model_cnn_ae.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training, make sure that the output of the autoencoder has the same shape as its input\n",
    "image_batch_example = next(iter(train_dataloader))[0]\n",
    "out = model_cnn_ae(image_batch_example)\n",
    "print(image_batch_example.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the training function.\n",
    "# It's similar to the MLP autoencoder (lab 4 - uses the ADAM optimizer), but images don't need to be vectorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - define the trainig/optimizer parameters: 50 epochs, MSE loss function, learning_rate=0.001\n",
    "# - train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualization: apply the autoencoder to a batch of images\n",
    "\n",
    "# Load it\n",
    "image_batch_example = next(iter(train_dataloader))[0]\n",
    "\n",
    "# Pass it to the autoencoder \n",
    "predicted_batch_example = model_cnn_ae_tr(image_batch_example).detach()\n",
    "\n",
    "# Plot the original and predicted images\n",
    "for ib in range(batch_size):\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_batch_example[ib, :].squeeze(), cmap='gray_r')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title('Original image')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(predicted_batch_example[ib, :].squeeze(), cmap='gray_r')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title('Predicted image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q3**</span> Put one of the plots above in your report.\n",
    "\n",
    "## Denoising autoencoder\n",
    "\n",
    "We now consider an application of autoencoders, which is image denoising. For this task, we can use the same CNN autoencoder model as before: the only thing that changes is the data on which it is trained.\n",
    "\n",
    "<center><a href=\"https://en.wikipedia.org/wiki/Total_variation_denoising\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/en/e/e8/ROF_Denoising_Example.png\"></a></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, here is a function that adds some noise to an input image batch\n",
    "def add_noise(inputs,noise_factor=0.8):\n",
    "     noisy = inputs+torch.randn_like(inputs) * noise_factor\n",
    "     noisy = torch.clamp(noisy,0.,1.)\n",
    "     return noisy\n",
    "\n",
    "# Add noise to the image_batch_example\n",
    "noisy_images_batch = add_noise(image_batch_example)\n",
    "\n",
    "# Plot the original and noisy images\n",
    "for ib in range(batch_size):\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_batch_example[ib, :].squeeze(), cmap='gray_r')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title('Original image')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(noisy_images_batch[ib, :].squeeze(), cmap='gray_r')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title('Noisy image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't want to create a new dataset, we will simply add noise to the images during training. Therefore, the training function should be the same as before, except we add noise to the images between passing them to the model. Then, we compute the loss between the (true) clean images and the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the training function for the denoising autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - instantiate CNN autoencoder and initialize its parameters\n",
    "# - define the training parameters: 50 epochs, MSE loss function, learning_rate=0.001\n",
    "# - train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualization: apply the denoising autoencoder to a batch of noisy images\n",
    "\n",
    "# Pass it to the autoencoder \n",
    "predicted_batch_example = model_cnn_den_ae_tr(noisy_images_batch).detach()\n",
    "\n",
    "# Plot the original and predicted images\n",
    "for ib in range(batch_size):\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image_batch_example[ib, :].squeeze(), cmap='gray_r')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title('Original image')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(noisy_images_batch[ib, :].squeeze(), cmap='gray_r')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title('Noisy image')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(predicted_batch_example[ib, :].squeeze(), cmap='gray_r')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title('Predicted image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q4**</span> Put one of the plots above in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
