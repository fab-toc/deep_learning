{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining a big CNN classifier\n",
    "\n",
    "This script simply defines and trains a large CNN classifier on the whole dataset for the students to use in lab 6.1 (introspection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process on: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import copy\n",
    "#from codecarbon import track_emissions\n",
    "\n",
    "# Define the data repository\n",
    "data_dir = 'data/'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Process on: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization function\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion MNIST dataset\n",
    "train_data = datasets.MNIST(data_dir, train=True, download=True, transform=transforms.ToTensor())\n",
    "num_classes = len(train_data.classes)\n",
    "\n",
    "# Split in training / validation\n",
    "n_train_examples = int(len(train_data)*0.9)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "train_data, valid_data = random_split(train_data, [n_train_examples, n_valid_examples])\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN classifier (with batch normalization)\n",
    "class CNNClassif_bnorm(nn.Module):\n",
    "    def __init__(self, input_size_linear, num_channels1=16, num_channels2=32, num_classes=10):\n",
    "        super(CNNClassif_bnorm, self).__init__()\n",
    "        \n",
    "        self.num_channels1 = num_channels1\n",
    "        self.num_channels2 = num_channels2\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.cnn_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, num_channels1, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_channels1),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "            \n",
    "        self.cnn_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(num_channels1, num_channels2, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_channels2),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        self.lin_layer = nn.Linear(input_size_linear, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.cnn_layer1(x)\n",
    "        out = self.cnn_layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.lin_layer(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval function\n",
    "def eval_cnn_classifier(model, eval_dataloader, device='cpu'):\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in eval_dataloader:\n",
    "            labels = labels.to(device)\n",
    "            images = images.to(device)\n",
    "            y_predicted = model(images).to(device)\n",
    "            _, label_predicted = torch.max(y_predicted.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (label_predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "#@track_emissions(offline=True, country_iso_code=\"FRA\")\n",
    "def training_val_cnn_classifier(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate, device='cpu', verbose=True):\n",
    "\n",
    "    # Make a copy of the model (avoid changing the model outside this function)\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    \n",
    "    # Set the model in 'training' mode (ensures all parameters' gradients are computed)\n",
    "    model_tr.train()\n",
    "    model_tr.to(device)\n",
    "    \n",
    "    # define the optimizer\n",
    "    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize lists to store the training loss and validation accuracy over epochs\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    val_acc_opt = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        tr_loss = 0\n",
    "        \n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "            # forward pass\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            y_predicted = model_tr(images).to(device)\n",
    "            loss = loss_fn(y_predicted, labels)\n",
    "\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the current epoch loss\n",
    "            tr_loss += loss.item() * images.shape[0]\n",
    "\n",
    "        # At the end of each epoch, get the average training loss and store it\n",
    "        tr_loss = tr_loss/len(train_dataloader.dataset)\n",
    "        train_losses.append(tr_loss)\n",
    "        \n",
    "        # Compute the accuracy on the validation set and store it\n",
    "        val_acc = eval_cnn_classifier(model_tr, valid_dataloader)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Display the training loss and validation accuracy\n",
    "        if verbose:\n",
    "            print('Epoch [{}/{}], Training loss: {:.4f} ; Validation accuracy: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, tr_loss, val_acc))\n",
    "            \n",
    "        # If the validation accuracy is higher than the optimal value, record the model and update the optimal value\n",
    "        if val_acc > val_acc_opt:\n",
    "            model_opt = copy.deepcopy(model_tr)\n",
    "            val_acc_opt = val_acc\n",
    "        \n",
    "    return model_opt, train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training loss: 0.1192 ; Validation accuracy: 98.3000\n",
      "Epoch [2/10], Training loss: 0.0520 ; Validation accuracy: 98.3667\n",
      "Epoch [3/10], Training loss: 0.0369 ; Validation accuracy: 98.8833\n",
      "Epoch [4/10], Training loss: 0.0282 ; Validation accuracy: 98.8167\n",
      "Epoch [5/10], Training loss: 0.0212 ; Validation accuracy: 98.8833\n",
      "Epoch [6/10], Training loss: 0.0183 ; Validation accuracy: 98.3000\n",
      "Epoch [7/10], Training loss: 0.0148 ; Validation accuracy: 99.1333\n",
      "Epoch [8/10], Training loss: 0.0127 ; Validation accuracy: 98.9667\n",
      "Epoch [9/10], Training loss: 0.0111 ; Validation accuracy: 99.1000\n",
      "Epoch [10/10], Training loss: 0.0095 ; Validation accuracy: 98.9167\n"
     ]
    }
   ],
   "source": [
    "# Network parameters\n",
    "num_channels1 = 16\n",
    "num_channels2 = 32\n",
    "num_classes = 10\n",
    "input_size_linear = 7*7*num_channels2 \n",
    "\n",
    "model = CNNClassif_bnorm(input_size_linear, num_channels1, num_channels2, num_classes)\n",
    "# Optimizer\n",
    "num_epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Training\n",
    "model_tr, loss_total, val_acc = training_val_cnn_classifier(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate, device=device, verbose=True)\n",
    "\n",
    "# Record the trained model\n",
    "model_tr.to('cpu')\n",
    "torch.save(model_tr.state_dict(), 'model_cnn_classif_introspection.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
